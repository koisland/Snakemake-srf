from typing import Any
from os.path import join, splitext, abspath, dirname


LOG_DIR = config.get("log_dir", "logs")
BMK_DIR = config.get("benchmark_dir", "benchmarks")
OUTPUT_DIR = config.get("output_dir", "results")


def get_parameters() -> tuple[dict[str, Any], dict[str, Any]]:
    samples = {}
    parameters = {}

    for sm, cfg in config["samples"].items():
        if cfg.get("input_dir"):
            infiles = cfg["input_dir"]
        elif cfg.get("input_files"):
            infiles = cfg["input_files"]
        else:
            raise FileNotFoundError("No input directory of files provided.")

        parameters[sm] = cfg["parameters"]
        samples[sm] = infiles

    return samples, parameters


SAMPLES, PARAMS = get_parameters()


wildcard_constraints:
    sm="|".join(SAMPLES.keys()),
    fname=r"[^\/]+",


rule compile_srf:
    output:
        directory(join(OUTPUT_DIR, "srf")),
    log:
        abspath(join(LOG_DIR, "compile_srf.log")),
    shell:
        """
        git clone https://github.com/lh3/srf {output} 2> {log}
        cd {output} && make &>> {log}
        """


rule compile_trf:
    output:
        bn=join(OUTPUT_DIR, "TRF-mod", "trf-mod"),
    log:
        abspath(join(LOG_DIR, "compile_trf.log")),
    params:
        outdir=lambda wc, output: dirname(output[0]),
    shell:
        """
        git clone https://github.com/lh3/TRF-mod {params.outdir} 2> {log}
        cd {params.outdir} && make -f compile.mak &>> {log}
        """


checkpoint get_fa_srf:
    input:
        infile=lambda wc: SAMPLES[wc.sm],
    output:
        outdir=directory(join(OUTPUT_DIR, "{sm}", "fasta")),
    log:
        join(LOG_DIR, "get_fa_srf_{sm}.log"),
    conda:
        "envs/tools.yaml"
    shell:
        """
        mkdir -p {output}
        # Get all non-hidden files.
        for file in $(find {input.infile} -mindepth 1 -maxdepth 1 -not -path '*/.*'); do
            # https://tldp.org/LDP/LG/issue18/bash.html 
            new_fname={output.outdir}/$(basename "${{file%%.*}}")".fa"
            if [[ ${{file}} == *fasta.gz || ${{file}} == *fa.gz ]]; then
                zcat ${{file}} > ${{new_fname}} 2>> {log}
            elif [[ ${{file}} == *.fasta || ${{file}} == *.fa ]]; then
                ln -s $(realpath ${{file}}) ${{new_fname}} 2>> {log}
            fi
        done
        """


rule count_high_occurence_kmers:
    input:
        infile=join(OUTPUT_DIR, "{sm}", "fasta", "{fname}.fa"),
    output:
        tmp_dir=temp(directory(join(OUTPUT_DIR, "{sm}", "{fname}", "temp"))),
        kmer_db_pre=join(OUTPUT_DIR, "{sm}", "{fname}", "count.kmc_suf"),
        kmer_db_suf=join(OUTPUT_DIR, "{sm}", "{fname}", "count.kmc_pre"),
        kmer_counts=join(OUTPUT_DIR, "{sm}", "{fname}", "count.txt"),
    threads: config.get("threads", 16)
    params:
        output_prefix=lambda wc, output: splitext(output.kmer_db_pre)[0],
        # Kmer size.
        k=lambda wc: PARAMS[wc.sm].get("kmer_size", 151),
        # Exclude kmers occurring less than n times.
        ci=lambda wc: PARAMS[wc.sm].get("exclude_kmers_lt_n", 3),
        # Maximal value of counter.
        cs=100_000,
    resources:
        mem="20GB",
    log:
        join(LOG_DIR, "count_high_occurence_kmers_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "count_high_occurence_kmers_{sm}_{fname}.txt")
    conda:
        "envs/tools.yaml"
    shell:
        """
        mkdir -p {output.tmp_dir}
        kmc -fm -k{params.k} -t{threads} \
            -ci{params.ci} -cs{params.cs} \
            {input.infile} {params.output_prefix} {output.tmp_dir} &> {log}
        kmc_tools transform {params.output_prefix} dump {output.kmer_counts} &>> {log}
        """


rule get_srf_motifs:
    input:
        bn_dir=rules.compile_srf.output,
        counts=rules.count_high_occurence_kmers.output.kmer_counts,
    output:
        motifs=join(OUTPUT_DIR, "{sm}", "{fname}", "srf.fa"),
    conda:
        "envs/tools.yaml"
    resources:
        mem="20GB",
    log:
        join(LOG_DIR, "get_srf_motifs_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "get_srf_motifs_{sm}_{fname}.txt")
    shell:
        """
        ./{input.bn_dir}/srf -p prefix {input.counts} > {output.motifs} 2> {log}
        """


rule get_srf_monomer:
    input:
        bn=rules.compile_trf.output.bn,
        motifs=rules.get_srf_motifs.output,
    output:
        monomers=join(OUTPUT_DIR, "{sm}", "{fname}", "monomers.tsv"),
    conda:
        "envs/tools.yaml"
    resources:
        mem="20GB",
    log:
        join(LOG_DIR, "get_srf_monomer_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "get_srf_monomer_{sm}_{fname}.txt")
    shell:
        """
        ./{input.bn} {input.motifs} | awk -v OFS="\\t" '{{ print "{wildcards.fname}", $0 }}' > {output} 2> {log}
        """


rule map_srf_to_source:
    input:
        bn_dir=rules.compile_srf.output,
        seq=join(OUTPUT_DIR, "{sm}", "fasta", "{fname}.fa"),
        motifs=rules.get_srf_motifs.output,
    output:
        paf=join(OUTPUT_DIR, "{sm}", "{fname}", "srf.paf"),
        enlong_contigs=join(OUTPUT_DIR, "{sm}", "{fname}", "srf_enlong.fa"),
    params:
        max_secondary_alns=lambda wc: PARAMS[wc.sm].get(
            "mm2_max_secondary_alns", 1_000_000
        ),
        ignore_minimizers_n=lambda wc: PARAMS[wc.sm].get(
            "mm2_ignore_minimizers_n", 1000
        ),
        aln_bandwidth=lambda wc: PARAMS[wc.sm].get("mm2_aln_bandwidth", "100,100"),
    threads: config.get("threads", 16)
    resources:
        mem="60GB",
    conda:
        "envs/tools.yaml"
    log:
        join(LOG_DIR, "map_srf_to_source_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "map_srf_to_source_{sm}_{fname}.txt")
    shell:
        """
        ./{input.bn_dir}/srfutils.js enlong {input.motifs} > {output.enlong_contigs} 2> {log}
        minimap2 -c \
            -N{params.max_secondary_alns} \
            -f{params.ignore_minimizers_n} \
            -r{params.aln_bandwidth} \
            -t{threads} \
            {output.enlong_contigs} {input.seq} > {output.paf} 2>> {log}
        """


rule create_bed:
    input:
        bn_dir=rules.compile_srf.output,
        paf=rules.map_srf_to_source.output.paf,
    output:
        bed=join(OUTPUT_DIR, "{sm}", "{fname}", "srf.bed"),
    conda:
        "envs/tools.yaml"
    log:
        join(LOG_DIR, "create_bed_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "create_bed_{sm}_{fname}.txt")
    shell:
        """
        {{ ./{input.bn_dir}/srfutils.js paf2bed {input.paf} | sort -k 1,1 -k2,2n ;}} > {output.bed} 2> {log}
        """


def srf_output(wc):
    outdir = checkpoints.get_fa_srf.get(**wc).output[0]
    fnames = glob_wildcards(join(outdir, "{fname}.fa")).fname
    return expand(rules.create_bed.output, sm=wc.sm, fname=fnames)


def trf_output(wc):
    outdir = checkpoints.get_fa_srf.get(**wc).output[0]
    fnames = glob_wildcards(join(outdir, "{fname}.fa")).fname
    return expand(rules.get_srf_monomer.output, sm=wc.sm, fname=fnames)


rule merge_files_n_cleanup:
    input:
        bed=srf_output,
        monomers=trf_output,
        # Cleanup temp dir.
        tmp_dir=rules.get_fa_srf.output,
    output:
        bed=join(OUTPUT_DIR, "{sm}", "srf.bed"),
        monomers=join(OUTPUT_DIR, "{sm}", "monomers.tsv"),
    shell:
        """
        sort -k1,1 -k2,2n {input.bed} > {output.bed}
        sort -k1,1 {input.monomers} > {output.monomers}
        rm -rf {input.tmp_dir}
        """


rule all:
    input:
        expand(rules.merge_files_n_cleanup.output, sm=SAMPLES),
    default_target: True
