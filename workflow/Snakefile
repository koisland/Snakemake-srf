import re
import os
from os.path import join, splitext, abspath, basename


LOG_DIR = config.get("log_dir", "logs")
BMK_DIR = config.get("benchmark_dir", "benchmarks")
OUTPUT_DIR = config.get("output_dir", "results")
RGX_FA = re.compile(r"^([^\.]+)\.(fasta|fa)(\.gz)*$")


def get_fasta_files():
    input_dir = config.get("input_dir")
    input_files = config.get("input_files")
    samples = {}

    if input_dir:
        infiles = [join(input_dir, file) for file in os.listdir(input_dir)]
    elif input_files:
        infiles = input_files
    else:
        raise FileNotFoundError("No input directory of files provided.")

    for file in infiles:
        mtch = re.search(RGX_FA, file)
        if mtch:
            sm = basename(mtch.group(1))
            samples[sm] = file
    return samples


SAMPLES = get_fasta_files()


wildcard_constraints:
    sm="|".join(SAMPLES.keys()),


rule compile_srf:
    output:
        directory(join(OUTPUT_DIR, "srf")),
    log:
        abspath(join(LOG_DIR, "compile_srf.log")),
    shell:
        """
        git clone https://github.com/lh3/srf {output} 2> {log}
        cd {output} && make &>> {log}
        """


rule decompress_infile:
    input:
        infile=lambda wc: SAMPLES[wc.sm],
    output:
        temp(join(OUTPUT_DIR, "{sm}", "temp.fa")),
    shell:
        """
        zcat {input} > {output}
        """


rule count_high_occurence_kmers:
    input:
        infile=lambda wc: (
            rules.decompress_infile.output
            if SAMPLES[wc.sm].endswith("gz")
            else SAMPLES[wc.sm]
        ),
    output:
        tmp_dir=temp(directory(join(OUTPUT_DIR, "{sm}", "temp"))),
        kmer_db_pre=join(OUTPUT_DIR, "{sm}", "count.kmc_suf"),
        kmer_db_suf=join(OUTPUT_DIR, "{sm}", "count.kmc_pre"),
        kmer_counts=join(OUTPUT_DIR, "{sm}", "count.txt"),
    threads: config.get("threads", 16)
    params:
        output_prefix=lambda wc, output: splitext(output.kmer_db_pre)[0],
        # Kmer size.
        k=config.get("kmer_size", 151),
        # Exclude kmers occurring less than n times.
        ci=config.get("exclude_kmers_lt_n", 3),
        # Maximal value of counter.
        cs=100_000,
    log:
        join(LOG_DIR, "count_high_occurence_kmers_{sm}.log"),
    benchmark:
        join(BMK_DIR, "count_high_occurence_kmers_{sm}.txt")
    conda:
        "envs/tools.yaml"
    shell:
        """
        mkdir -p {output.tmp_dir}
        kmc -fa -k{params.k} -t{threads} \
            -ci{params.ci} -cs{params.cs} \
            {input.infile} {params.output_prefix} {output.tmp_dir} &> {log}
        kmc_tools transform {params.output_prefix} dump {output.kmer_counts} &>> {log}
        """


rule get_srf_contigs:
    input:
        bn_dir=rules.compile_srf.output,
        counts=rules.count_high_occurence_kmers.output.kmer_counts,
    output:
        contigs=join(OUTPUT_DIR, "{sm}", "srf.fa"),
    conda:
        "envs/tools.yaml"
    log:
        join(LOG_DIR, "get_srf_contigs_{sm}.log"),
    benchmark:
        join(BMK_DIR, "get_srf_contigs_{sm}.txt")
    shell:
        """
        ./{input.bn_dir}/srf -p prefix {input.counts} > {output.contigs} 2> {log}
        """


rule map_srf_to_source:
    input:
        bn_dir=rules.compile_srf.output,
        seq=lambda wc: SAMPLES[wc.sm],
        contigs=rules.get_srf_contigs.output,
    output:
        paf=join(OUTPUT_DIR, "{sm}", "srf.paf"),
        enlong_contigs=join(OUTPUT_DIR, "{sm}", "srf_enlong.fa"),
    params:
        max_secondary_alns=config.get("mm2_max_secondary_alns", 1_000_000),
        ignore_minimizers_n=config.get("mm2_ignore_minimizers_n", 1000),
        aln_bandwidth=config.get("mm2_aln_bandwidth", "100,100"),
    threads: config.get("threads", 16)
    conda:
        "envs/tools.yaml"
    log:
        join(LOG_DIR, "map_srf_to_source_{sm}.log"),
    benchmark:
        join(BMK_DIR, "map_srf_to_source_{sm}.txt")
    shell:
        """
        ./{input.bn_dir}/srfutils.js enlong {input.contigs} > {output.enlong_contigs} 2> {log}
        minimap2 -c \
            -N{params.max_secondary_alns} \
            -f{params.ignore_minimizers_n} \
            -r{params.aln_bandwidth} \
            -t{threads} \
            {output.enlong_contigs} {input.seq} > {output.paf} 2>> {log}
        """


rule create_bed:
    input:
        bn_dir=rules.compile_srf.output,
        paf=rules.map_srf_to_source.output.paf,
    output:
        bed=join(OUTPUT_DIR, "{sm}", "srf.bed"),
    conda:
        "envs/tools.yaml"
    log:
        join(LOG_DIR, "create_bed_{sm}.log"),
    benchmark:
        join(BMK_DIR, "create_bed_{sm}.txt")
    shell:
        """
        {{ ./{input.bn_dir}/srfutils.js paf2bed {input.paf} | sort -k 1,1 -k2,2n ;}} > {output.bed} 2> {log}
        """


rule merge_and_slop_bed:
    input:
        fai=lambda wc: SAMPLES[wc.sm] + ".fai",
        bed=rules.create_bed.output,
    output:
        bed=join(OUTPUT_DIR, "{sm}", "satellite_region.bed"),
    conda:
        "envs/tools.yaml"
    params:
        bp_merge=config.get("bed_merge", 500_000),
        bp_slop=config.get("bed_slop", 500_000),
    log:
        join(LOG_DIR, "merge_and_slop_{sm}.log"),
    benchmark:
        join(BMK_DIR, "merge_and_slop_{sm}.txt")
    shell:
        """
        {{ bedtools merge -i {input.bed} -d {params.bp_merge} | \
        bedtools slop -i - -g {input.fai} -b {params.bp_slop} ;}} > {output} 2> {log}
        """


rule all:
    input:
        expand(rules.map_srf_to_source.output, sm=SAMPLES),
        expand(rules.create_bed.output, sm=SAMPLES),
        expand(rules.merge_and_slop_bed.output, sm=SAMPLES),
    default_target: True
