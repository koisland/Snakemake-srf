from typing import Any
from os.path import join, splitext, abspath


LOG_DIR = config.get("log_dir", "logs")
BMK_DIR = config.get("benchmark_dir", "benchmarks")
OUTPUT_DIR = config.get("output_dir", "results")


def get_parameters() -> tuple[dict[str, Any], dict[str, Any]]:
    samples = {}
    parameters = {}

    for sm, cfg in config["samples"].items():
        if cfg.get("input_dir"):
            infiles = cfg["input_dir"]
        elif cfg.get("input_files"):
            infiles = cfg["input_files"]
        else:
            raise FileNotFoundError("No input directory of files provided.")

        parameters[sm] = cfg["parameters"]
        samples[sm] = infiles
    
    return samples, parameters


SAMPLES, PARAMS = get_parameters()


wildcard_constraints:
    sm="|".join(SAMPLES.keys()),
    fname=r"[^\/]+",


rule compile_srf:
    output:
        directory(join(OUTPUT_DIR, "srf")),
    log:
        abspath(join(LOG_DIR, "compile_srf.log")),
    shell:
        """
        git clone https://github.com/lh3/srf {output} 2> {log}
        cd {output} && make &>> {log}
        """


checkpoint get_fa_srf:
    input:
        infile=lambda wc: SAMPLES[wc.sm]
    output:
        outdir=directory(join(OUTPUT_DIR, "{sm}", "fasta"))
    log:
        join(LOG_DIR, "get_fa_srf_{sm}.log"),
    conda:
        "envs/tools.yaml"
    shell:
        """
        mkdir -p {output}
        # Get all non-hidden files.
        for file in $(find {input.infile} -mindepth 1 -maxdepth 1 -not -path '*/.*'); do
            # https://tldp.org/LDP/LG/issue18/bash.html 
            new_fname={output.outdir}/$(basename "${{file%%.*}}")".fa"
            if [[ ${{file}} == *fasta.gz || ${{file}} == *fa.gz ]]; then
                zcat ${{file}} > ${{new_fname}} 2>> {log}
                samtools faidx ${{new_fname}} 2>> {log}
            elif [[ ${{file}} == *.fasta || ${{file}} == *.fa ]]; then
                ln -s $(realpath ${{file}}) ${{new_fname}} 2>> {log}
                samtools faidx ${{new_fname}} 2>> {log}
            fi
        done
        """


rule count_high_occurence_kmers:
    input:
        infile=join(OUTPUT_DIR, "{sm}", "fasta", "{fname}.fa"),
    output:
        tmp_dir=temp(directory(join(OUTPUT_DIR, "{sm}", "{fname}", "temp"))),
        kmer_db_pre=join(OUTPUT_DIR, "{sm}", "{fname}", "count.kmc_suf"),
        kmer_db_suf=join(OUTPUT_DIR, "{sm}", "{fname}", "count.kmc_pre"),
        kmer_counts=join(OUTPUT_DIR, "{sm}", "{fname}", "count.txt"),
    threads: config.get("threads", 16)
    params:
        output_prefix=lambda wc, output: splitext(output.kmer_db_pre)[0],
        # Kmer size.
        k=lambda wc: PARAMS[wc.sm].get("kmer_size", 151),
        # Exclude kmers occurring less than n times.
        ci=lambda wc: PARAMS[wc.sm].get("exclude_kmers_lt_n", 3),
        # Maximal value of counter.
        cs=100_000,
    log:
        join(LOG_DIR, "count_high_occurence_kmers_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "count_high_occurence_kmers_{sm}_{fname}.txt")
    conda:
        "envs/tools.yaml"
    shell:
        """
        mkdir -p {output.tmp_dir}
        kmc -fa -k{params.k} -t{threads} \
            -ci{params.ci} -cs{params.cs} \
            {input.infile} {params.output_prefix} {output.tmp_dir} &> {log}
        kmc_tools transform {params.output_prefix} dump {output.kmer_counts} &>> {log}
        """


rule get_srf_contigs:
    input:
        bn_dir=rules.compile_srf.output,
        counts=rules.count_high_occurence_kmers.output.kmer_counts,
    output:
        contigs=join(OUTPUT_DIR, "{sm}", "{fname}","srf.fa"),
    conda:
        "envs/tools.yaml"
    log:
        join(LOG_DIR, "get_srf_contigs_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "get_srf_contigs_{sm}_{fname}.txt")
    shell:
        """
        ./{input.bn_dir}/srf -p prefix {input.counts} > {output.contigs} 2> {log}
        """


rule map_srf_to_source:
    input:
        bn_dir=rules.compile_srf.output,
        seq=join(OUTPUT_DIR, "{sm}", "fasta", "{fname}.fa"),
        contigs=rules.get_srf_contigs.output,
    output:
        paf=join(OUTPUT_DIR, "{sm}", "{fname}", "srf.paf"),
        enlong_contigs=join(OUTPUT_DIR, "{sm}", "{fname}", "srf_enlong.fa"),
    params:
        max_secondary_alns=lambda wc: PARAMS[wc.sm].get("mm2_max_secondary_alns", 1_000_000),
        ignore_minimizers_n=lambda wc: PARAMS[wc.sm].get("mm2_ignore_minimizers_n", 1000),
        aln_bandwidth=lambda wc: PARAMS[wc.sm].get("mm2_aln_bandwidth", "100,100"),
    threads: config.get("threads", 16)
    conda:
        "envs/tools.yaml"
    log:
        join(LOG_DIR, "map_srf_to_source_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "map_srf_to_source_{sm}_{fname}.txt")
    shell:
        """
        ./{input.bn_dir}/srfutils.js enlong {input.contigs} > {output.enlong_contigs} 2> {log}
        minimap2 -c \
            -N{params.max_secondary_alns} \
            -f{params.ignore_minimizers_n} \
            -r{params.aln_bandwidth} \
            -t{threads} \
            {output.enlong_contigs} {input.seq} > {output.paf} 2>> {log}
        """


rule create_bed:
    input:
        bn_dir=rules.compile_srf.output,
        paf=rules.map_srf_to_source.output.paf,
    output:
        bed=join(OUTPUT_DIR, "{sm}", "{fname}", "srf.bed"),
    conda:
        "envs/tools.yaml"
    log:
        join(LOG_DIR, "create_bed_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "create_bed_{sm}_{fname}.txt")
    shell:
        """
        {{ ./{input.bn_dir}/srfutils.js paf2bed {input.paf} | sort -k 1,1 -k2,2n ;}} > {output.bed} 2> {log}
        """


rule merge_and_slop_bed:
    input:
        fai=join(OUTPUT_DIR, "{sm}", "fasta", "{fname}.fa.fai"),
        bed=rules.create_bed.output,
    output:
        bed=join(OUTPUT_DIR, "{sm}", "{fname}", "satellite_region.bed"),
    conda:
        "envs/tools.yaml"
    params:
        # Filter HORs that fall below one monomer.
        bp_filter=lambda wc: PARAMS[wc.sm].get("kmer_size", 151),
        bp_merge=lambda wc: PARAMS[wc.sm].get("bed_merge", 500_000),
        bp_slop=lambda wc: PARAMS[wc.sm].get("bed_slop", 500_000),
    log:
        join(LOG_DIR, "merge_and_slop_{sm}_{fname}.log"),
    benchmark:
        join(BMK_DIR, "merge_and_slop_{sm}_{fname}.txt")
    shell:
        """
        {{ awk -v OFS="\\t" '$7>={params.bp_filter}' {input.bed} | \
        bedtools merge -i - -d {params.bp_merge} | \
        bedtools slop -i - -g {input.fai} -b {params.bp_slop} ;}} > {output} 2> {log}
        """

def srf_output(wc):
    outdir = checkpoints.get_fa_srf.get(**wc).output[0]
    fnames = glob_wildcards(join(outdir, "{fname}.fa")).fname
    return expand(rules.merge_and_slop_bed.output, sm=wc.sm, fname=fnames)


rule merge_bedfiles_n_cleanup:
    input:
        bed=srf_output,
        # Cleanup temp dir.
        tmp_dir=rules.get_fa_srf.output,
    output:
        join(OUTPUT_DIR, "{sm}", "regions.bed")
    shell:
        """
        cat {input.bed} > {output}
        rm -rf {input.tmp_dir}
        """

rule all:
    input:
        expand(rules.merge_bedfiles_n_cleanup.output, sm=SAMPLES),
    default_target: True
